{"version":3,"file":"static/js/593.94714b7b.chunk.js","mappings":"iIAAA,s+gD","sources":["components/pProject.md"],"sourcesContent":["export default \"#p-project #p-value #statistics #decision-making #academia\\n\\n\\n## What is it?\\n\\nThe .p-project is an educational and informative initiative aimed at distributing unbiased and correct knowledge about statistics and its applications in academic setting for research. It is now well known and widespread that academic research, in some fields more than others, is lacking basic knowledge of statistical instruments or the skills for the interpretation of results.  \\nResearchers nowadays are overtrained on design features, and lack a basic comprehension of what this design is going to affect and where one’s decision could be read and debugged in the aftermath of a data analysis process.\\nAnother reason it’s the incredible and fast spread of noxious information, which is polluting the environment, especially online, of good and reliable information. Even though on most websites, the basic foundations of a test are a well ordered series of “copy and paste” without a critical comment attached to them, there is often a little slimy incorrect information laying around somewhere in these blogs, websites, posts and so on.\\nWith the .p-project and for these reasons, I’m aiming to spread some of my critical insights and knowledge on a few fundamental aspects of both theory and practice of data analysis and research. \\nThe name of the initiative comes from the almighty p-value, that in the last 70 years has transcended its mere mathematical boundaries into a nigh omnipotent mind controlling God. Veteran scientists DO NOT know what a p-value is, how to interpret it, and what kind of conclusions draw from its existence after their automatized series of clicks on the statistical software has given it birth.  \\nMost of them comprehend factors that might influence their measurements, but have absolutely no clue as to how that bends data analysis results.\\n\\n## The p-value\\n\\nThe p-value is an important tool in the field of statistics and is used to make inferences and draw conclusions about population parameters based on sample data. This index is the typical result of what are widely known as tests of statistical significance. There are a vast array of these types of tests, one example being the popular t-test, which was developed by Sir William Gosset in 1908 and published in the journal Biomedika.\\nHowever the first appearance of the term “P” to indicate the result of a significance test can be backtracked to one of the first and earliest publications of Ronald Fisher, that we can confidently define, the “inventor” of the modern scientific method and science. The paper titled “On the Interpretation of χ2 from Contingency Tables, and the Calculation of P” published in the Journal of the Royal Statistical Society in 1922, was the first one to mention this value of p as an actual measure of a significance test. \\nHowever, as many authors before us tend to notice, the term significance in general and the idea of comparing means and variability of randomly sampled groups, has its roots as far back as the ancient Greece. Was it Cicero in his celeber dialogue “De divinatione” who was implying that a difference of a certain magnitude could not be attributed to casualty.\\n\\n\\n> “*Casu, inquis. Itane vero? Quicquam potest casu esse factum, quod omnes habet in se numeros veritatis? Quattuor tali iacti casu Venerium efficiunt; num etiam centum Venerios, si quadringentos talos ieceris, casu futuros putas? Adspersa temere pigmenta in tabula oris liniamenta efficere possunt; num etiam Veneris Coae pulchritudinem effici posse adspersione fortuita putas? Sus rostro si humi A litteram impresserit, num propterea suspicari poteris Andromacham Enni ab ea posse describi? Fingebat Carneades in Chiorum lapicidinis saxo diffisso caput exstitisse Panisci; credo, aliquam non dissimilem figuram, sed certe non talem, ut eam factam a Scopa diceres. Sic enim se profecto res habet, ut numquam perfecte veritatem casus imitetur.*”\\n\\n  \\n\\n“By chance, you’re saying? But can really happen by chance, something that has in itself all the characteristics of truth? Four dices, thrown randomly, are giving a “Venus sign”; but if you threw four hundred dices, and you obtained a Venus sign at least a hundred times, would you believe that a chance? [...] Some colors splattered by chance on a drawing board can remind the lines of a face; but would you believe that randomly splattering colors you might obtain the beauty of the Venus of Coo? If a pig drew an “a” with his mouth on the soil, would you believe him being able to write Ennio’s Andromaca? [...] This is how things are, there’s no doubt: Chance shall never be able to imitate the Truth perfectly.”\\n\\nZiliak and McCloskey identify this first appearance of the significance way of thinking, and in their famed book “The Cult of Statistical Significance” they broadly address the topic. However, I’m not quite sure they have fully exhausted this particular reference to the core, considering they took a translated version of the latin masterpiece. With a more strict translation one could argue that Cicerone was not indeed laying the foundation for significance, but perhaps displaying an insane level of statistical insights (for that time) that would be useful even to the vast majority of researchers nowadays. With the thirteenth paragraph (not the 23rd as Ziliak and McCloskey report in their book), Cicerone, through his character Quintus (his brother), is basically making a case for repetition and patterns of occurrences.\\nHe basically understood the p-value before it was even invented or mathematically formalized. Here we arrive at one of the biggest incorrect beliefs about the p-value, given by the classical rhyme “p-value is the probability of something happening not only by chance alone”. This particular idea has come in various shapes and forms, but the substance of it all is this. The incorrect belief that the p-value has something to do with the epistemological nature of a test, answering the question of “did I find something?”. \\nAlthough it’s true that chance is a data generating process, if we can call it this way, it is also true that we cannot attribute the result of an experiment to chance itself. How could we? Considering how many steps are governed and controlled by humans, it is more likely that fluctuations or noise in our data could derive from human error (which is not chance).\\nBut here we arrive at the first paragraph of our journey: what is the p-value really?\\n\\n## Origin of the p-value: William Sealy Gosset\\n\\nThe actual inventor of a lot of our statistical tools for hypothesis testing is Sir William Gosset, who was mainly a brewer working for Guinness (not the world record, but the beer company). He was indeed a scientist, but not an academic. He published some articles under the pseudonym of Student. The Student’s t test, takes its name from him. He was the first to formalize the z-score and t-score tables from which all the modern tests of significance reference for critical values and thus for p-values.\\nThe reason why Gosset “invented” or better, discovered this relationship in the random sampling behavior, was because he was interested in a method that could aid him in terms of decision making when dealing with very small samples, for the purpose his main job. Working with an actual company, he was called to decide which stock of barley or hop was better for brewing. Since that was the prime matter for beer, the company could not afford to give him large samples to draw conclusions from, hence small samples decisions. However, Gosset was never concerned with the “did I find something?” question. He believed that the more important question was “how much?”. Basically for him quantifying and estimating was the way to go in order to implement informed decisions, especially in a commercial business. \\nBut if that was his concern, where does the current concept of the p-value come from? Well, contrary to him, Ronald Fisher, another important and prominent statistician of the first half of the 20th century, believed and transformed the important question into a philosophical matter.\\nSo, since Sir Ronald Fisher believed that the p-value could answer the question about the existence or not of a true effect, what did Sir Gosset instead believed? He simply believed that the p-value was not able to answer such a question, and he later availed the methodological framework of Eagon Pearson (the son of Karl Pearson), who believed that the scientific method was a tradeoff between two possible types of errors. But, on the subject of the p-value, there is no possibility on earth that the index could mitigate the uncertainty of a true effect being there or not (as frequentists would say, the data generating process).\\n\\n## What is the p-value?\\n\\nThe p-value is none other than an assigned (ascribed) probabilistic value that has been stripped of its probabilistic nature. In his first publication for the journal Biometrika, Sir Gosset (“The probable error of the mean” 1925) delines the distribution of z, the now known z-score. He does that by sampling random features of reality, such as finger length distribution, and noted that the more numerous the sample was, the more a distribution (that was indeed normal) would resemble normality. He accounted for the error of lesser distribution inferred from smaller samples. So, when someone computes a p-value, that is none other than the PDF (probability density function) ascribed to such “lesser” distributions, it comes by default that with a higher numerosity and a smaller standard deviation you have a higher p-value. However, this comes from a pre-built distribution which has (possibly) nothing to do with the actual measurements of our modern day experiment.\\nSo, this is not a discount of the mathematical genius of Sir Gosset or Sir Ronald Fisher, because for what we know today, the p-value reconcile with the alpha (type I error) on very big sampling numbers (see later), but just a beware of the nature of the index itself.\\nContrary to the belief of many, the p-value is not an actual probability, since both the frequentist and the Bayesian view of a probability entails the central paradigm of repetition or observation (realization of the random variable). That is not actually happening in any experiment computing a p-value, but what is actually computed is the effect size, or the actual mean of the sample (with the associated standard deviation, which is, in my opinion, the actual important index to look for).\\nAnother common disbelief is the threshold of significance, which are basically some absolutely arbitrary points which do not have anything to do with the whole picture. The .05 is not a magic number below which our findings become relevant, and it is not a magic threshold at which our uncertainty becomes certainty. There is absolutely no trust to be put in the .05 or the .01 thresholds (there is also the .001, which is the minimum required for hard sciences), and nothing to gain by claiming a discovery having descended below that. But still, researchers fail to understand that, blinded and tormented by the question “how can we say that we have found something then?”. Well, isn’t the question you’re asking yourself wrong in the first place? This is where, for example, a machine learning approach at a philosophical level could aid a lot of statistics.\\nIn model building through machine learning we are not worried about why something works in a certain way, we are only concerned about decision making. And for decision making we want to know how well the model predicts reality, from that point we can also infer and postulate mechanistic relationships between parts of the model and ad finem, suppose why something behaves a certain way. The question that has been lost, or the focus has shifted from, “what can I do with it?” to “how am I sure?”. But isn’t a model capable of replicating reality, mechanistically embedded by definition? And we are back with the reflection of Cicerone, and how he claimed that if something repeatedly acts the same way, with only a few errors, that is indeed something true without the need to ask an index what to think or to answer the question for ourselves.\\nThus said, would I bet my money on the smaller p-value if presented with a range of values? Yes. But would I do the same if I were presented with a range of effect sizes? Yes, of course. However, this is not the article where I speak about decision making. In conclusion for this section, p-value must be treated as it is, an added value that could synthesize the magnitude of samples and standard deviation estimation, but not measure of certainty or a threshold.\\n\\n\\n# Simulations and the behavior of .p\\n\\n\\n## Sampling from 2 different distributions\\n\\nI tried to reproduce some experiments published in the recent decade, about the p-value. All the simulations where originally performed in the R language, whilst I reproduced them in Python. The first experiment I tried to reproduce was the one from McCloskey of 2015, which has been also criticized for the conclusions and erroneously addressed. I will address the critique later on, for the moment I will hereby present a reproduction of the experiment, with my personal take and a different adjustment of the whole simulation. It is also possible to find the complete notebook with all the code I used in my repository on GitHub.\\n\\nFirst of all, I’ve used a random generator process in order to sample an **n** amount of samples from a normal distribution. The endeavor to replicate this experiment with a fully randomized distribution, incorporating the representation of noise and other real-life occurrences, is a commendable objective from a scientific standpoint.\\nSo two samples drawn from two different populations. The first sample, of 1000 observations, was drawn from a population with a real mean of 0 and a real standard deviation of 1. It is important to stress out the “real” adjective, because when researchers perform studies they have only estimates of the mean and the standard deviation. So, as far as we are concerned, we could go on and call the standard deviation, standard error. For the sake of simplicity, we will keep addressing them as mean (m) and standard deviation (sd). The second sample of the same numerosity, the same standard deviation (this is important) and just a 0.5 bigger mean. Now, I want you to keep in mind this, as the most important part of the simulation, since working with these fixed parameters gives us the capability of having a lot of random samples, but gives us a unique and specific scenario of premises.\\nSo for the first part we have the two distributions plotted with a fitted curve to help us assess for normality. Usually in research there are tests that, for example, rely on significance testing to assess for normality. I’m not stretching enough how this is not optimal on so many levels, but I will just add that a graphical assessment, especially with samples so big and the fact that we drew our samples from a script that is actually generating normal distributions, we do not need a separate test for normality.\\nAs we can see from Figure 1, the distribution is exactly normal and the mean of the two samples differs exactly for 0.5. After this, the follow up task was to build up four different dataset, which entailed different measures, ranging from confidence intervals, effect sizes, p-values and the log10 transformation of p-values. Four different samples sizes have been chosen in order to control for power:\\n\\n- n=10, with a theoretical power of 18%;\\n- n=30, with a theoretical power of 48%;\\n- n=60, with a theoretical power of 78%;\\n- n=100, with a theoretical power of 95%;\\n\\nNow, the simulation part took place. I performed 1000 t-tests and stored the results previously mentioned in 4 different datasets depending on the sample size. There are some details that I want to emphasize, that were not stretched out or presented vividly enough in the article from McCloskey that in my opinion mark a pretty important accent on the “scenario” argument we presented in the beginning.\\n\\n1. The t-tests were performed between samples with the same number of observations, even though considering the formula I implemented it was not necessary.\\n2. Following, the effect size estimate utilized was Cohen's D, which is a raw effect size that on equal standard deviations is none other than a difference between the two sample means. This is of crucial importance considering the premises we made in the beginning.\\n\\nBoth different standard deviations and different sample sizes, which are an occurrence in research settings (for sample sizes, you have designs that force you to have even samples, but that’s not the case every time), would affect the results.\\nPlus, I won’t stress this enough, we are basically creating perfect conditions for all other experimental biases that could potentially occur in a real life scenario and limit the validity of our prediction (meaning that a researcher would have a lower chance to detect a real effect size).\\nAfter all this is considered, we have a distribution for the p-values results and I’ve transformed the values in a log10 function which is more of an intuitive visualization. Figures from 2 to 5 are representative of the distributions of the p-values.\\n\\nThe results yielded this:\\n\\n- 230 significant results for the samples with 10 observations;\\n- 347 significant results for the samples with 30 observations;\\n- 807 significant results for the samples with 60 observations;\\n- 959 significant results for the samples with 100 observations;\\n\\nThis is a similar result to what obtained by McCloskey and I would have not expected otherwise. Here, there’s something important to point out, and that is the fact that this is NOT the behavior of an individual p-value, but is a virtual repetition of 1000 experiments. This is unheard of in real life, and has brought non other than mathematical confirmation that the p-value, on the long run, agrees with the alpha or (type I error rate).\\n\\nA particular observation of this sort has been done by van Helden in a publication on Nature Methods of 2016. He goes on saying that “the p-value does its job”. \\nThis observation is true but only in the particular instance in which we are drawing samples from two different populations. The behavior is indeed completely different when we sample from the same population as it is explained further in the paper. Unfortunately, a researcher does not have the faculty to know beforehand and in general, it is impossible to know what's the real nature of the two samples in terms of which population they belong to. But, at this point, we could argue that the job is “done better” by the smaller p-value and that we would not need any p-value at all since by default, when we set up the two error rates from the design, we are about sure that on a large repetition of experiments we could get to a certain number of false positives and false negatives.  Eventually, repetition is the key attitude to implement in academia, and giving up a strict guide that would damage us in the long run is not a bad consideration at all. One must realize that having a thousand repetitions of the same experiment is also not feasible in real life, so the original question of \\\"what is a good indicator of true effect\\\" still stands.\\nBut let’s go back first to the examples where the sampling occurs from two perfectly different populations, as we intended to do in the first place.\\n\\nLet’s consider the case with 95% theoretical power. Theoretical and experimental power broadly agree, what we are witness on the bar plots is none other than 1 = Power + Beta, which is the cumulative probability being the alternative hypothesis true (i.e. the 1).\\nObserving my graphs you can also see two other vertical lines, which represent the .01 and .001 levels of significance. If we still look at the 100 observations distribution we can clearly see that those levels predict an even smaller number of significant values. So, here’s when I want to invite you to be alert and think: how is it possible that a stricter p-value is worse at predicting true positive results? \\n\\nAnd this is just the second half of the coin, but again, we are considering a scenario that almost never occurs in a real world setting, due to a lot of other factors involved. So yes, eventually an effect exists or not, but this is not what we are getting as a “message” from the data. If we use the p-value as a guide to discern what the data is telling us, we are likely to make a lot of wrong decisions.\\nIn one simple question here we see the “broadly agreement” argument. The reason is, that the threshold of the p-value is just arbitrary, while the actual alpha level is part of a probabilistic framework which posits that depending on degrees of freedom and standard deviation estimates from the population, accepting a certain rate of false positives and false negatives will ultimately lead to specific error rates. In table 1 we can clearly see how the p-value of 0.5 is broadly outperforming all other threshold and not only that, we can see a linear trend of how to a higher p-value threshold correspond a lower rate of false negatives, becoming practically non existent with higher thresholds and high samples.\\n\\nSo, van Helden, who simulated 10000 (an order of magnitude above our experiment) t-tests but just with a sample size of 30, proceeds to repeat the same simulation sampling from two populations with an exact same mean. Adding to this the same standard deviation is basically implying that you would get almost two perfectly equal samples, rendering all the work a futile “proof” that has little to no application or utility in a real life setting.\\nThis is the paradox of the deus ex machina, all knowing, that draws conclusion from a completely unrealistic perspective. So, even if we can technically say that in super specific circumstances the p-value \\\"agrees\\\", if repeated a n number of times, with alpha, on the other side this is practically useless when we, as decision making agents, are in front of the first p-value yielded by the first experiment. That value could arbitrarily be coming from the same population (no effect) or from two different populations (real effect) but unless we preemptively know that - which is paradoxically and ironically the reason we are making a decision on the first p-value - there is no way to tell if that significant p-value is worth betting on.  \\n\\nBut let’s argue from the (wrong) point of view of van Helden. In a real life scenario (we are still not bringing noise and design flaws in the picture) you could not tell which one of the p-values is part of the group of the “correct” p-values as we have just pointed out. Eventually the reality of it all is that we can increase our chances of reducing both false positives and false negatives by increasing the number of observations, but eventually the p-value threshold and the specific p-value has no utility whatsoever on a single experiment scenario (could arbitrarily be coming from same or different populations) and it's useless on repetitions (eventually the error rates framework will do it's job)\\n\\n## Confidence intervals\\n\\nThe confidence intervals are usually referred to as a measure of precision of the estimate. They are expressed as the 90% or 95% confidence intervals. What they are actually expressed as, is the 95% interval of a measure or an estimate. Plenty of misinterpretation of the confidence intervals have been portrayed in literature and in general there is an erroneous comprehension of the concept of the confidence interval. There have been suggestions from researchers to change the “confidence” part of the name, with something less contradictory. Confidence intervals seem to suggest that there is a level of confidence that you can stake on the measure. This paired with a percentage number is the reason as to why the widespread notion of confidence intervals, indicating the amount of confidence you can place in a measure, has been nurtured. Another common misconception which is however just a slight misunderstanding of the true nature of the confidence interval, is the concept of the CI being the probability of the true value falling within the interval. Although this is the actual meaning of the confidence interval intended as a frequentist parameter of statistical nature, which sees its realization in repeated sampling, it is not the case for the single confidence interval.\\n\\nThe single confidence interval that is built from a single point estimate, such an effect size, is just a parametrization of a range, based on some characteristic of the estimated sample distribution. There is nothing revelatory or epistemological about the single confidence interval, as much as there is not for the p-value. \\nHowever, the simulation at stake managed to show how the distribution of confidence intervals is behaving on repeated sampling, with different sample sizes. In figure 3 we have a simple portrait of the four confidence intervals distribution simulated from different sample sizes (which were the same as the t-tests for significance). To obtain a distribution of values, the single confidence interval — since of every pair of samples drawn from population A and B, it was repeated n=1000 times — is being measured as a gap (e.g. a confidence interval of -2,+1 is being measured as |3| ). This allows us to create 3 different distributions of CIs around the mean difference obtained by the t-tests (CI of the effect size).\\nIt is clear in figure 3 we can appreciate how the numerosity of the samples affect the shape of the distribution, which resembles normality since the data generation process was indeed normal anyways (i.e. Central Limit Theorem). The distribution from the n=100 samples is much steeper and has thinner tails. All the values are extremely centralized and hover around the true value of the mean. \\nFigure 4 gives an in depth perception of the clustering of the confidence intervals and point estimates of the t-test (µA - µB), which is showing how the mean of the interval distribution is however always centered around the true estimate of the difference between the population means.\\nThis is suggesting that regardless of the amplitude of the confidence intervals, on average, the repetition of the study with different samples but with the same parameters (in this case the alpha rate set around .05) will eventually give us 95% of the true positive rate, which is none other than the 95% confidence intervals set (or distribution). The supposed precision of the estimate is not at all of relevance here, considering that regardless of the range of the distributions, the mean of the point estimates is centering around the true value.\\nImportant insights are also given by counting how many confidence intervals from the four relative distributions to sample sizes actually contain the true value of the mean (true effect size). This would give a better explanation and a definitive answer to the question of the confidence intervals being a measure of precision or not. It will also make a strong case for more numerous samples being or not a better “confidence” in terms of precision (i.e. if one could potentially say that a CI of an effect size from two bigger samples is in any way more precise, by default, of one resulting from smaller samples differences). The values of the number of CI containing the true value are displayed in Table 2. We can clearly see that there is not an actual difference between samples. The confidence intervals display exactly what is their core function, ~95% of the true value of the difference between the mean. In the first column we have the results from the current example, the second column, we can see how it is the same even if one were to sample from two identical populations. This is enticing as to witness, because regardless of the null hypothesis being true or false, the repeated nature of confidence intervals is finding itself being conserved. \\nFrom figure 5 instead we can see an informative graphical representation of the confidence intervals gap distribution, against the point distribution of the values and their respective level of significance. On the left, we have the distributions which once again are showing centering behavior towards the true value. On the right side the scatter plot displays the shaded area with the non-significant values. As previous pieces of information have shown, and in agreement with Table 1. The shaded area diminishes with the increased numerosity. Also, one could observe how the points all narrow around the true value difference respecting the same tendency as the numerosity increases. Under these specific conditions, this seems to be the closest representation of the concept of precision, which still is not prescinding from a necessity of repetition.\\nIt would be interesting to add a simulation that defines what is the minimum amount of repetitions needed to have the proportion of the confidence intervals respected, with respect to the reference sampling distribution. Meaning that, if one were to repeat a t-test 10 times, how many CI would contain the true value? Will the sample size still be irrelevant in terms of precision? Further studies are necessary to investigate this aspect of the confidence intervals.\\n\\n## Sampling from two similar distributions\\n\\nThe simulation was repeated by changing the parameters of the two distributions. This second set of trials is to underline the differences that might arise, in the behavior of the most popular statistical metrics, if one were to sample from two identical populations. This is the equivalent to saying that the null hypothesis is indeed true, meaning that the difference between the two means of the populations is non-existent. The importance of making a comparison between these two sets of trials will be clear later in the text. \\nPopulation A will have the same parameters as the first trial, whilst population B won’t have a 0.5 difference in the mean, but will have the exact same parameters as population A, making the null hypothesis (H0) indeed true. The standard deviation will be set again to 1. The number of trials repeated for each t-test will be again of n=1000. In figure 6 we can again see that the two population distributions this time are exactly equal. A graphical representation of the two populations is still necessary to understand how the parameters shape the distributions of the data, and that the mean of the populations is exactly overlapping. The first simulation is a repetition of the first experiment, where for each sample numerosity (e.g. 10,30,60,100) a run of 1000 t-tests has been conducted. In table 4 there is the aggregate data, displayed at each level of arbitrary significance threshold, to compare how each threshold is performing against each other. This time the smaller threshold is behaving exactly like it should, it is displaying the least amount of significant results. The .001 threshold is performing the best. The trend is preserved going to the right of the table, where the numerosity is increasing the effect by showing even more stringent numbers of significant results.  \\nIt is wise to remember that the significant results shown in this table are none other than false positives. So the p-value here is allowing a certain number of false positives depending on the arbitrary threshold we admitted. Another consideration, which is important to stress out, considering is a common misconception at academic level, is the fact that the alpha type I error rate does not conflate conceptually with the significance threshold. Later in the text it will be shown how, instead, the confidence intervals will be displaying a very high level of overlap with the concept of type I error rate. In table 1 in fact we already displayed the data that showed that approximately, on 1000 trials (the mathematical formality is generalized ad infinitum) ~950 confidence intervals contain the true value of the difference between the means (meaning that the 50 remaining trials, or 5%, were false positives). This was also true for two different populations, as it was already discussed previously. \\nAnother thing to consider from the significance thresholds is that the true positives, instead, are a “variable” realized concept of power instead. It is well known how the power of a study is something that is defined a-priori conceptually speaking. Usually 80% is considered a high level power study, while the remaining 20% is left to the false negatives. The reason why this study has implemented the same metric as the Hesley et al study, is because of the simplicity of the parameters in order to simulate, and to understand the concepts. Plus, the four sample numerosities display a fixed level of theoretical power for the difference we want to detect, in both scenarios. Since the actual power, from repeated sampling, was ~95% regardless of the sample numerosity, we are also questioning whether or not the subdivision in null and alternative distribution is indeed necessary.\\nThe null hypothesis has gained the odd cousin known as the alternative hypothesis very recently in history, being framed into the picture just with the advent of the NHSF. Of course, the choice of the power parameter is being done on a theoretical basis, because in that case one does not know if they are going towards a true null or a false null. However, still the simulations show how the confidence intervals in repetitions detect most of the true value of the effect size to a degree well above what theoretical sample numerosity power would suggest.\\nFrom figure 7 we can see how this time the relative distributions to numerosity of significant values are very less pronounced. Again, in figure 8 the overlap of the distributions of the .95 gaps shows how, regardless of sample numerosity, the distributions center around the true effect size (which is 0 in this case). \\nIn figure 9 the grid is showing again the scatter to significance level of all the individual tests for numerosity, out of 1000 trials. Here the difference is clearly staggering with the first grid, which was showing how there was a narrowing of the CI gaps, going towards higher numerosities but a reduction in significance, with a false null hypothesis (sampling from two different populations). Here we are seeing what we would expect to see also in the previous case, a narrowing of the scatter with a decrease in significance. \\nSo, eventually up until this point, the data would suggest to not trust the p-value in general, regardless of the threshold and regardless of the numerosity of the sample. Moreover, there is an added conundrum that comes into being when the null hypothesis is false, which is that the lower the threshold of the p-value, the more confidence one is putting into a higher rate of potentially false negatives (which was way beyond 90% in some cases).\\n\\n# S-value as “cognitive” metric\\n\\nIn the paper from Rafi & Greenland 2020 it has been proposed a different metric in order to simplify cognitive appraisal of study results for researchers. This type of proposal has stemmed from the simple argument that is presented as a crucial point as to why a different metric and cognitive descriptors are needed: “Most researchers lack the time or skills for re- education, so we need methods that are simple to acquire quickly based on what is commonly taught, yet are also less vulnerable to common misinterpretation than are traditional approaches (or at least have not yet become as widely misunderstood as those approaches).” Another point deeming the highlight is, quoting: “Most researchers are under pressure to produce definitive conclusions, and so will resort to familiar automated approaches and questionable defaults, with or without P-values or “statistical significance.”\\nFollowing these reasonable argumentations, easily experienceable by most researchers, there is the proposal of transforming the infamous p-value into a more interpretable s-value, which is none other than the -log2 transformation of said parameter. This would yield a value whose unit of measure is in bits, basically the rate of information. Rafi and Greenland go on claiming how this could be interpreted as information against the model, which is basically a restatement of the previously proposed “compatibility” definition of the p-value.\\nLet’s start from this and unravel what is there to be avoided from a cognitive point of view. First of all, as from this work itself, p-value cannot be interpreted as compatibility nor tell anything about the actual model. This is because the founding presumption is wrong, such as “is there an effect?”, which is commonly overused and implicitly asserted by virtually all papers published, even if it’s not a consistent framework of thought onto which to base any given experiment.\\nThis is due to the fact that the p-value itself is not able to answer epistemological questions. So the first big critical assumption of the modern scientific method is generally a 50% coin toss. Assuming the null hypothesis is true, as to say “ assuming there is no real effect”, this data is incompatible with my model, as the p-value is trying to tell me.\\nHowever, the null hypothesis being true, is just an assumption which is a very long shot. As we have seen from the section with the simulations, assuming no effect when there is in fact an effect, would lead a stricter p-value in a higher rate of false negatives, and a lesser rate of true positives. It is also reasonable to foster the idea that in a real life scenario, with real data and experiments, one would have at disposal previous experimental results onto which assumptions can be made and null hypotheses can be generated. This would not only add a considerable degree of subjectivity in the designing part, which is inherently a double edged sword, but would also raise a distressing question mark with regards to the original assumption. Basically the first ever experiment on the subject, which is nowadays rarely seen in literature (most power calculations are based on “similar” studies) is going to fall in the perspective of a complete subjective bias (a coin toss decision). Adding to all this, the reasoning we are latching onto is of a clear Bayesian footprint.\\n\\nNow, how does that translate for the s-value, which is none other than a transformed p-value? First of all, from the proposal we can see that the threshold has been eliminated, which is a good step forward. The threshold brought absolutely no value to the testing framework or to the interpreting capabilities of a researcher. This because it was just the progeny of a wrong mix of two different approaches to science (i.e. Fisherian framework, and the Neyman-Pearson framework). \\nPutting this out of the way, we have an associate reading in bits thrown at us every time we estimate an effect size. What would be the frame of reference for this value? Absolutely no frame of reference. But somebody at some point should come along and establish at least some reference values, exactly like it has been done for Cohen’s d in social sciences (e.g. we know that a .8 effect size is a large effect, considering common knowledge).\\nBut will this make any difference?\\nIf we still consider the single experiment, getting away with 1.3 bits or 12.3 bits of information, doesn’t tell us anything really. It just tells us that there’s allegedly “more information” against the model. Not changing however the fact that we are still in the 50% limbo of uncertainty that we might have just found or not found a real effect. The problem still being the fact that we are reasoning in terms of proving that something exists rather than measuring and estimating phenomena. \\nHow can we safely assess that 12.3 bits of information (totally arbitrary value) is information against our model, and not in favor of our model? Simply, we have no clue. Sure, it is a step forward considering that the vast majority of scientists believe that the p-value is the probability of being right or wrong. \\nHowever, testing out some of the transformations, I’ve run two separate experiments. The first one sampling from two different populations, and the second one sampling from the same population.\\n\\n## Experiment 1: sampling from two different populations\\n\\nThe first part of this experiment dealt with two different populations. For the sake of consistency the population parameters were left unchanged from previous parts (i.e. mean of the population 1 = 0 and mean of the population 2 = 0.5, both with a standard deviation of 1). Four sample sizes, also in this case to be consistent with previous parts of this work, were used to simulate 1000 t-tests, which yielded 1000 p-values that were then transformed following the -log2 Shannon’s transformation.\\nWe then have obtained our dataset with 1000 s-values at four different sample sizes: \\n\\n- N = 10\\n- N = 30\\n- N = 60\\n- N = 100\\n\\nFollowing Fari and Greenland reasoning, in this particular case, since we already know that we are sampling from two different populations, considering a null hypothesis of “no effect”, it could be affirmed that we have a cumulative amount of information against the null. This is because, since the unit of measure of the s-value is bits, it’s only logical to consider cumulative information against the model as, per se, cumulative.\\nThen we are left with:\\n\\n- A total of 2647.45 bits of information for the 10 samples;\\n- A total of 4143.28 bits of information for the 30 samples;\\n- A total of 8332.52 bits of information for the 60 samples;\\n- A total of 13301.38 bits of information for the 100 samples;\\n\\nThis seems to be in line with the nature of the p-value, which scales with the sample size based on what curve of the t-distribution (considering the degrees of freedom) the effect size falls upon. However, here the framework is considering all the information as a quantitative amount of “proof” that the data are not compatible with the model or the design in toto.\\nHowever, we have to consider that we are still behaving in a scenario when we have an unrealistic amount of tests, no measurement error, no noise from other sources, and actually a perfectly even repetition of these tests. Plus we are arguing from a position when we know the data generating processes, such as the fact that we are sampling from two different populations, which in reality, whoever the researcher may be, she doesn’t know. If we observe Figure 1 carefully however we can see that the 4 curves that represent the different sample sizes of the simulations, are converging at the highest point, from a value that is close to 0, but they don’t touch each other in between.\\nThis is an interesting behavior, meaning that it could be a potential indicator than a different original behavior of p-values that could guide us to choose which scenario we encountered (i.e. sampled from the same population or from two different populations).\\n\\n## Experiment 2: sampling from the same population\\n\\nThe second time I did nothing other than recreate the same scenario as the first experiment, but sampling from the same population, which for clarity was population 1 ( mean = 0 ). The sample sizes are always the same four:\\n\\n- N = 10\\n- N = 30\\n- N = 60\\n- N = 100\\n\\nIn this case, however, we see how the first difference arises from the cumulative value of information that we have against the null hypothesis. This time the null hypothesis is technically the same, but we know we are sampling from the same population. Given a real life scenario, also in this case we would reject the null hypothesis following consecutive experiments or just some of them, because at this point the s-value indicates information against the null. And given that we do not have a frame of reference we basically do not have any element to decide whether our null is appropriate or not.\\nThe cumulative levels of information in bits, sampling from the same populations are as follows:\\n\\n- A total of 1458.03 bits of information for the 10 samples;\\n- A total of 1408.91 bits of information for the 30 samples;\\n- A total of 1456.20 bits of information for the 60 samples;\\n- A total of 1363.66 bits of information for the 100 samples;\\n\\nSo, as we can see, there’s not a real difference or increment from one level of degrees of freedom to another, even when the change is basically in the order of almost doubling every time the number of observations. We cannot detect a clear trend but the assumption of almost remaining the same could be made in specific conditions.\\nObserving Figure 2, one can clearly see how this time the curves are basically overlapping, and we expected that. Consider that, as for Figure 1, the values of s are being sorted in ascending order, so we wouldn’t have such a precise overlap with the raw data, but this gives more of an indication to us as to how different are the rates of information we are getting sampling from the same population.\\nIt can be hypothesized that the rate of gain of information is basically increased, in the case of us sampling from two different populations, if we increase the degrees of freedom. This is a safe and easy deduction to make if we are basically dealing with perfect samples, perfect sampling and a scenario where we already know what we are dealing with and how the sampling works. It would be different in a real world scenario, where most of these practices are not perfect and knowledge is unknown.\\n\\n### Truth in repetition: what is currently lacking\\n\\nAn apology to current research and academic practices is the fact that most of the experiments nowadays are not repeated. Basically nobody is trying to seek confirmation after the results of another’s experiment. This has been highlighted as a problem by countless practitioners in research, but just a few changes have been sighted.\\nBack to our main problem, from the previous paragraph we are still lacking the frame of reference or the rational basis on which to consider a result more suitable to convey an inferential conclusion of the epistemological nature ( is there a real effect or not? ). The solution could be in the rate of information gain.\\nSo I wrote a program that simulated 1000 t-tests from a seed population, normally distributed, with the same characteristics of the previous simulations. This time the program would simulate 1000 t-tests starting from samples with N = 0 going forward. For the sake of clarity, the first few comparisons at N = 0,1,2 were basically nulls given that you could not calculate standard deviations or other population parameters.\\nWhat the program is trying to achieve is a comparison of two different scenarios: sampling from two different populations, and sampling from the same population. We can see in Figure 3, which is the result output of the program, how this simulation went. We are watching at the rate of gain of information, expressed in bits from the s-values yielded, and two different curves. The first one being the increasing one, representing the t-tests for two different populations, and the horizontal one being basically spurring from t-tests between samples drawn from the same population.\\nIt is important to consider how the rate of gain of information is increasing, given that we are increasing the degrees of freedom. This would also bypass the scenario of not knowing if we are sampling from the same or different population, because we would not have a rising rate in the first case. However, this type of verification would still be susceptible to all other types of errors and bad practices, especially noise or measurement errors.\\nHowever there is a lane of applicability of this type of awareness, being it this one: experiment could be repeated with the same null, and at different levels of numerosity. For example, repeated at the same sample size as the original, and then increasing by a fixed factor (yet to be decided the optimal level).\\nEven if this wouldn’t be applicable in all types of designs, I’ve limited the program N between 0 and 60. This is a range that is frequently seen in literature, in terms of sample size numerosity.\\nThe key however, lies in the repetition of the experiments. The s-value solution is a good addition to the toolbox of a researcher in order to assess the results of her experiment, but we still have to consider that the best information we can get is the estimation. After the estimation, the “how much?” we can safely try to answer the question of “is there a real effect?”. This can only be done by repeating the experiments.\\n\";"],"names":[],"sourceRoot":""}